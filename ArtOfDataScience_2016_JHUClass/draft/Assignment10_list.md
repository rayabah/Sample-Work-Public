## Group:
  * What is the spike? More Visualizations,
                     Regressions ,
                     Clustering ,
                     PCA
  * Bic keeps getting better as we go up right now--look at more clusters!
  * Look at histograms of the x,y bics from last time
  * Sample 100 or 1000 samples and rerun x,y bic curve after sampling (run bic many times, take max for each k)
    because fragile to outliers
  * Look at the two kmeans center of clusters graphs in the z-layer. Better visualizations, which way do they trend?
  * Covariances from last hw as heat maps

## Emily:
  * Plot vector estimates on top of the z-layer cluster center graph to see if estimate makes sense.
  * Visualization of the "spike" (scatter plot, etc)
  * Redo z-layer bics without setting random state (take max at each k) and analyze results
  * Work off of jay's gradient of density estimate (not using uniques), compare the two
  * Visualization of clustering after running the bic curve for higher ks (kmeans)
  * In the spike:
   * Min/Max x-value
   * Min/Max y-value
   * Min/Max z-value
   * Max number of synapses at one point
   * Point at which max number of synpases occurs

## Bijan:
#### 5 solo things hw week 4/4/2016
  * Overlay the scatterplot and vector graphs to find relationships - they seemed to correlate.
  * Find the direction between cluster centers per each z value, plot them all together and compare.
  * Run what I did last week, creating direction of increasing density, using # of partitions as a hyperparameter. Compare data across # of partitions (in x, y, z).
  * From the BIC curves generated last week, analyze the data from the z values that behaved differently (bend at 4 instead of 3). Plot histograms + cluster.
  * Elastic net regression on predicting synapses from xyz data for each z value, especially the ones that behaved differently earlier.

## Akash:
  * Regression in cluster 1 and 4
  * Regression in cluster 1,2,3
  * Scale centroid charts to be equal sizes
  * Compare centroid values in the 3 and 4 cluster groups
  * Figure out how to apply point cloud
  
  

## Jay:
1. Write code to generate simulated data using a probability distribution model of our data. The model will be as follows: for each block of space (that is row of data) the number of synapses follows a binomial distribution with parameters p=synapses/unmasked and n=unmasked, so data generated by this model will have the same set of coordinates and unmasked values as the true data but number of synapses will differ (I don't know very much about Monte Carlo simulation, but from what I do know, I think this would be considered a Monte Carlo simulation, or atleast something similar?). Since the number of 'trials' (unmasked voxels) is so high, it may be necessary to approximate the binomial distributions with Gaussians or Poissons.
 
2. Run the simulation many times (for example, n=10,000) and then use the results to estimate the mean number of synapses (theoretically should be very similar to the true number of synapses since E[Bin(p,n)]=pn); results can also be used to compute a confidence interval and p-value (given alpha).
 
3. Using the same results estimate the variance and compute confidence interval and p-value. 

4. Can a simpler, but still accurate, model be found? For example, if we n find clusters of data such that within each cluster, synapses/unmasked is approximately equal for all data points, then a new model could be made with n binomial distributions, one for each cluster, as opposed to the original model which uses ~30,000 (approx. number of rows after 'cleaning' data) binomial distributions. 

5. Run two sample K-S tests comparing the true data with the samples obtained through simulation (obtained in part 2). If we find that the samples generated by the model are in fact not similar to the true data, then what wrong assumptions does this model make? Also, if a simpler model(s) is found from part 4, we can also run the same tests on samples generated by the simplified model to see how well the simplified model approximates the more complex one from part 1.
